# üö® Microservices Edge Cases & Error Handling - Expert Guide

> **M·ª•c ti√™u**: N√¢ng level t·ª´ Mid-level ‚Üí Senior/Staff Engineer th√¥ng qua vi·ªác x·ª≠ l√Ω c√°c edge cases, failure scenarios v√† production-grade patterns.

---

## üìå M·ª•c l·ª•c

1. [Distributed Transactions & Edge Cases](#1-distributed-transactions--edge-cases)
2. [Kafka: Error Handling & DLQ Patterns](#2-kafka-error-handling--dlq-patterns)
3. [Idempotency: Duplicate Request Handling](#3-idempotency-duplicate-request-handling)
4. [Network Failures & Partial Failures](#4-network-failures--partial-failures)
5. [Database Edge Cases](#5-database-edge-cases)
6. [Race Conditions & Concurrency](#6-race-conditions--concurrency)
7. [API Design: Error Response Patterns](#7-api-design-error-response-patterns)
8. [Distributed Tracing & Debugging](#8-distributed-tracing--debugging)
9. [Chaos Engineering & Testing](#9-chaos-engineering--testing)
10. [Production Checklist](#10-production-checklist)

---

## 1. Distributed Transactions & Edge Cases

### 1.1. Saga Pattern - C√°c Edge Cases Th∆∞·ªùng G·∫∑p

#### Edge Case 1: Compensation Failure (Rollback c·ªßa Rollback)

```
Scenario:
1. Order Service ‚Üí t·∫°o ƒë∆°n h√†ng ‚úì
2. Inventory Service ‚Üí tr·ª´ stock ‚úì
3. Payment Service ‚Üí thanh to√°n ‚úó (th·∫•t b·∫°i)
4. Compensation: Inventory Service ‚Üí ho√†n stock ‚úó (c≈©ng th·∫•t b·∫°i!)
```

**Gi·∫£i ph√°p Senior/Staff:**

```java
@Service
@Slf4j
public class SagaCompensationHandler {
    
    private final RetryTemplate retryTemplate;
    private final CompensationLogRepository compensationLog;
    private final AlertService alertService;
    
    /**
     * Compensation v·ªõi retry v√† fallback
     */
    public void compensateWithRetry(CompensationTask task) {
        try {
            retryTemplate.execute(ctx -> {
                log.info("Compensation attempt {} for task {}", 
                    ctx.getRetryCount(), task.getId());
                executeCompensation(task);
                return null;
            }, ctx -> {
                // Recovery callback - sau khi h·∫øt retry
                handleCompensationFailure(task, ctx.getLastThrowable());
                return null;
            });
        } catch (Exception e) {
            // Critical: Compensation th·∫•t b·∫°i ho√†n to√†n
            saveToManualInterventionQueue(task, e);
            alertService.sendCriticalAlert(
                "SAGA_COMPENSATION_FAILED",
                Map.of("taskId", task.getId(), "error", e.getMessage())
            );
        }
    }
    
    /**
     * L∆∞u v√†o queue ƒë·ªÉ x·ª≠ l√Ω th·ªß c√¥ng
     */
    private void saveToManualInterventionQueue(CompensationTask task, Exception e) {
        CompensationLog log = CompensationLog.builder()
            .taskId(task.getId())
            .sagaId(task.getSagaId())
            .compensationType(task.getType())
            .payload(task.getPayload())
            .errorMessage(e.getMessage())
            .status(CompensationStatus.REQUIRES_MANUAL_INTERVENTION)
            .createdAt(Instant.now())
            .build();
        
        compensationLog.save(log);
    }
}
```

#### Edge Case 2: Saga Coordinator Dies Mid-Transaction

```
Scenario:
1. Saga Orchestrator b·∫Øt ƒë·∫ßu transaction
2. Step 1, 2 ho√†n th√†nh
3. Orchestrator crash/restart
4. Step 3, 4 ch∆∞a th·ª±c hi·ªán ‚Üí H·ªá th·ªëng inconsistent
```

**Gi·∫£i ph√°p: Saga State Persistence**

```java
@Entity
@Table(name = "saga_instances")
public class SagaInstance {
    
    @Id
    private String sagaId;
    
    @Enumerated(EnumType.STRING)
    private SagaState state; // STARTED, STEP_1_COMPLETED, STEP_2_COMPLETED, etc.
    
    @Column(columnDefinition = "jsonb")
    private String context; // To√†n b·ªô data c·∫ßn thi·∫øt ƒë·ªÉ resume
    
    private Instant startedAt;
    private Instant lastUpdatedAt;
    
    @Version
    private Long version; // Optimistic locking
}

@Service
public class ResilientSagaOrchestrator {
    
    private final SagaInstanceRepository sagaRepo;
    
    /**
     * Resume incomplete sagas sau khi restart
     */
    @Scheduled(fixedDelay = 60000) // M·ªói ph√∫t
    @Transactional
    public void recoverIncompleteSagas() {
        Instant timeout = Instant.now().minus(5, ChronoUnit.MINUTES);
        
        List<SagaInstance> stuckSagas = sagaRepo
            .findByStateNotInAndLastUpdatedAtBefore(
                List.of(SagaState.COMPLETED, SagaState.COMPENSATED),
                timeout
            );
        
        for (SagaInstance saga : stuckSagas) {
            log.warn("Recovering stuck saga: {}", saga.getSagaId());
            resumeSaga(saga);
        }
    }
    
    private void resumeSaga(SagaInstance saga) {
        SagaContext context = deserialize(saga.getContext());
        
        switch (saga.getState()) {
            case STEP_1_COMPLETED:
                executeStep2(saga, context);
                break;
            case STEP_2_COMPLETED:
                executeStep3(saga, context);
                break;
            // N·∫øu state l√† STEP_X_FAILED ‚Üí trigger compensation
            case STEP_2_FAILED:
                compensateStep1(saga, context);
                break;
            default:
                // Log v√† alert
                break;
        }
    }
}
```

#### Edge Case 3: At-Least-Once Delivery + Non-Idempotent Operations

```
Scenario:
1. Payment Service nh·∫≠n message "deduct $100"
2. X·ª≠ l√Ω xong, g·ª≠i ACK
3. ACK b·ªã m·∫•t do network issue
4. Kafka resend message
5. Payment deduct th√™m $100 l·∫ßn n·ªØa!
```

**Gi·∫£i ph√°p: Idempotency v·ªõi Deduplication Store**

```java
@Service
public class IdempotentPaymentProcessor {
    
    private final RedisTemplate<String, String> redis;
    private static final long DEDUP_TTL_HOURS = 24;
    
    @KafkaListener(topics = "payments")
    public void processPayment(ConsumerRecord<String, PaymentEvent> record) {
        String messageId = record.headers()
            .lastHeader("message-id")
            .value()
            .toString();
        
        String dedupKey = "payment:processed:" + messageId;
        
        // Ki·ªÉm tra ƒë√£ x·ª≠ l√Ω ch∆∞a (atomic operation)
        Boolean isNew = redis.opsForValue()
            .setIfAbsent(dedupKey, "1", Duration.ofHours(DEDUP_TTL_HOURS));
        
        if (Boolean.FALSE.equals(isNew)) {
            log.info("Duplicate message detected, skipping: {}", messageId);
            return; // Skip duplicate
        }
        
        try {
            doProcessPayment(record.value());
        } catch (Exception e) {
            // X√≥a key ƒë·ªÉ c√≥ th·ªÉ retry
            redis.delete(dedupKey);
            throw e;
        }
    }
}
```

### 1.2. Outbox Pattern - Edge Cases

#### Edge Case: Outbox Poll Race Condition

```
Scenario:
1. Instance A: SELECT ... FOR UPDATE t·ª´ outbox (row 1, 2, 3)
2. Instance A: ƒêang g·ª≠i message...
3. Instance A: Crash tr∆∞·ªõc khi UPDATE status
4. Lock ƒë∆∞·ª£c release
5. Instance B: Poll l·∫°i row 1, 2, 3
6. Message b·ªã duplicate!
```

**Gi·∫£i ph√°p: Polling v·ªõi Lease**

```java
@Service
public class OutboxPoller {
    
    @Transactional
    public List<OutboxMessage> pollWithLease(String instanceId, int batchSize) {
        Instant now = Instant.now();
        Instant leaseExpiry = now.plus(2, ChronoUnit.MINUTES);
        
        // L·∫•y messages ch∆∞a c√≥ lease ho·∫∑c lease ƒë√£ expired
        List<OutboxMessage> messages = outboxRepo.findMessagesToProcess(
            now, batchSize
        );
        
        if (messages.isEmpty()) {
            return Collections.emptyList();
        }
        
        // G√°n lease cho instance n√†y
        for (OutboxMessage msg : messages) {
            msg.setLeasedBy(instanceId);
            msg.setLeaseExpiresAt(leaseExpiry);
        }
        
        outboxRepo.saveAll(messages);
        return messages;
    }
    
    @Transactional
    public void markAsProcessed(List<Long> messageIds, String instanceId) {
        // Double check lease ownership
        int updated = outboxRepo.updateStatusByIdsAndLeasedBy(
            messageIds, 
            OutboxStatus.PROCESSED,
            instanceId
        );
        
        if (updated != messageIds.size()) {
            log.warn("Some messages were already processed by another instance");
        }
    }
}
```

---

## 2. Kafka: Error Handling & DLQ Patterns

### 2.1. Error Handling Strategy Matrix

| Error Type | Retry? | DLQ? | Alert? | Example |
|------------|--------|------|--------|---------|
| **Transient** | ‚úì (with backoff) | After max retries | After DLQ | Network timeout, DB connection |
| **Poison Pill** | ‚úó | ‚úì Immediately | ‚úì | Malformed JSON, schema mismatch |
| **Business Error** | Depends | ‚úì | ‚úó | Insufficient funds, item not found |
| **Infrastructure** | ‚úì (indefinite) | ‚úó | ‚úì Critical | Kafka broker down |

### 2.2. Production-Grade Error Handler

```java
@Configuration
public class KafkaErrorHandlerConfig {
    
    @Bean
    public DefaultErrorHandler errorHandler(
            KafkaTemplate<String, Object> kafkaTemplate,
            MeterRegistry meterRegistry) {
        
        // Retry configuration
        ExponentialBackOffWithMaxRetries backOff = 
            new ExponentialBackOffWithMaxRetries(5);
        backOff.setInitialInterval(1000);    // 1s
        backOff.setMultiplier(2.0);          // 1s, 2s, 4s, 8s, 16s
        backOff.setMaxInterval(30000);       // Max 30s
        
        // Error handler v·ªõi custom recovery
        DefaultErrorHandler handler = new DefaultErrorHandler(
            new DeadLetterPublishingRecoverer(
                kafkaTemplate,
                (record, ex) -> {
                    // Th√™m metadata v√†o DLQ message
                    return new TopicPartition(
                        record.topic() + ".DLQ", 
                        record.partition()
                    );
                }
            ),
            backOff
        );
        
        // Ph√¢n lo·∫°i error: retry vs kh√¥ng retry
        handler.addNotRetryableExceptions(
            JsonProcessingException.class,    // Poison pill
            ValidationException.class,         // Bad data
            IllegalArgumentException.class
        );
        
        handler.addRetryableExceptions(
            TransientDataAccessException.class,
            OptimisticLockingFailureException.class,
            SocketTimeoutException.class
        );
        
        // Metrics cho observability
        handler.setRetryListeners((record, ex, deliveryAttempt) -> {
            meterRegistry.counter("kafka.consumer.retry",
                "topic", record.topic(),
                "attempt", String.valueOf(deliveryAttempt)
            ).increment();
        });
        
        return handler;
    }
}
```

### 2.3. DLQ Processing Pattern

```java
@Service
@Slf4j
public class DLQProcessor {

    private final ObjectMapper objectMapper;
    private final DLQRepository dlqRepository;
    private final AlertService alertService;

    /**
     * Consumer cho DLQ topic
     */
    @KafkaListener(
        topics = "${kafka.topics.orders}.DLQ",
        groupId = "dlq-processor"
    )
    public void processDLQ(ConsumerRecord<String, String> record) {
        DLQMessage dlqMessage = DLQMessage.builder()
            .originalTopic(extractHeader(record, "original-topic"))
            .originalPartition(extractIntHeader(record, "original-partition"))
            .originalOffset(extractLongHeader(record, "original-offset"))
            .errorMessage(extractHeader(record, "exception-message"))
            .errorStacktrace(extractHeader(record, "exception-stacktrace"))
            .payload(record.value())
            .receivedAt(Instant.now())
            .status(DLQStatus.PENDING)
            .retryCount(0)
            .build();
        
        // Ph√¢n lo·∫°i error
        classifyAndProcess(dlqMessage);
    }
    
    private void classifyAndProcess(DLQMessage msg) {
        String errorMessage = msg.getErrorMessage();
        
        if (isPoisonPill(errorMessage)) {
            // Poison pill: l∆∞u ƒë·ªÉ review manual
            msg.setStatus(DLQStatus.REQUIRES_MANUAL_REVIEW);
            msg.setClassification("POISON_PILL");
            dlqRepository.save(msg);
            
            alertService.sendAlert(
                AlertLevel.WARNING,
                "DLQ Poison Pill detected",
                Map.of("topic", msg.getOriginalTopic(), "error", errorMessage)
            );
            
        } else if (isTransientError(errorMessage)) {
            // Transient: schedule retry
            msg.setStatus(DLQStatus.SCHEDULED_RETRY);
            msg.setClassification("TRANSIENT");
            msg.setNextRetryAt(Instant.now().plus(1, ChronoUnit.HOURS));
            dlqRepository.save(msg);
            
        } else {
            // Unknown: c·∫ßn investigation
            msg.setStatus(DLQStatus.REQUIRES_INVESTIGATION);
            msg.setClassification("UNKNOWN");
            dlqRepository.save(msg);
            
            alertService.sendAlert(
                AlertLevel.HIGH,
                "DLQ Unknown error type",
                Map.of("messageId", msg.getId(), "error", errorMessage)
            );
        }
    }
    
    /**
     * Scheduled job ƒë·ªÉ retry DLQ messages
     */
    @Scheduled(fixedDelay = 300000) // 5 ph√∫t
    public void retryScheduledMessages() {
        List<DLQMessage> toRetry = dlqRepository
            .findByStatusAndNextRetryAtBefore(
                DLQStatus.SCHEDULED_RETRY,
                Instant.now()
            );
        
        for (DLQMessage msg : toRetry) {
            if (msg.getRetryCount() >= MAX_DLQ_RETRIES) {
                msg.setStatus(DLQStatus.MAX_RETRIES_EXCEEDED);
                dlqRepository.save(msg);
                continue;
            }
            
            try {
                reprocessMessage(msg);
                msg.setStatus(DLQStatus.REPROCESSED_SUCCESS);
            } catch (Exception e) {
                msg.setRetryCount(msg.getRetryCount() + 1);
                msg.setNextRetryAt(calculateNextRetry(msg.getRetryCount()));
                msg.setLastError(e.getMessage());
            }
            
            dlqRepository.save(msg);
        }
    }
}
```

### 2.4. Kafka Consumer Edge Cases

#### Edge Case: Consumer Lag Spiraling

```
Scenario:
1. Consumer A x·ª≠ l√Ω ch·∫≠m (10 msg/s)
2. Producer g·ª≠i nhanh (100 msg/s)
3. Lag tƒÉng d·∫ßn ‚Üí Rebalance timeout
4. Rebalance ‚Üí Consumer m·∫•t assignment
5. Sau rebalance ‚Üí Lag c√≤n l·ªõn h∆°n!
```

**Gi·∫£i ph√°p: Dynamic Scaling + Backpressure**

```java
@Component
public class KafkaLagMonitor {
    
    private final AdminClient adminClient;
    private final ScalingService scalingService;
    
    @Scheduled(fixedDelay = 30000)
    public void monitorAndScale() {
        Map<TopicPartition, Long> endOffsets = getEndOffsets();
        Map<TopicPartition, Long> currentOffsets = getCurrentCommittedOffsets();
        
        long totalLag = 0;
        for (TopicPartition tp : endOffsets.keySet()) {
            long lag = endOffsets.get(tp) - currentOffsets.getOrDefault(tp, 0L);
            totalLag += lag;
            
            // Alert n·∫øu lag partition c·ª• th·ªÉ qu√° cao
            if (lag > PARTITION_LAG_THRESHOLD) {
                alertService.sendAlert(
                    AlertLevel.WARNING,
                    String.format("High lag on %s: %d", tp, lag),
                    Map.of("partition", tp.toString(), "lag", lag)
                );
            }
        }
        
        // Auto-scale d·ª±a tr√™n lag
        if (totalLag > SCALE_UP_THRESHOLD) {
            scalingService.scaleUp("order-consumer", 1);
        } else if (totalLag < SCALE_DOWN_THRESHOLD) {
            scalingService.scaleDown("order-consumer", 1);
        }
    }
}
```

#### Edge Case: Exactly-Once Processing Pitfall

```
Scenario:
1. Consumer ƒë·ªçc message, x·ª≠ l√Ω xong
2. Commit offset th√†nh c√¥ng
3. Ghi DB ‚Üí th·∫•t b·∫°i!
4. Message ƒë√£ committed nh∆∞ng ch∆∞a x·ª≠ l√Ω ‚Üí Data loss!
```

**Gi·∫£i ph√°p: Transactional Outbox + Manual Offset**

```java
@Service
public class TransactionalConsumer {
    
    @KafkaListener(
        topics = "orders",
        // T·∫Øt auto commit
        properties = {"enable.auto.commit=false"}
    )
    public void processWithTransaction(
            ConsumerRecord<String, OrderEvent> record,
            Acknowledgment ack) {
        
        // CRITICAL: X·ª≠ l√Ω v√† commit trong c√πng 1 DB transaction
        try {
            processOrderWithinTransaction(record.value(), record.offset());
            
            // Commit Kafka offset SAU KHI DB commit th√†nh c√¥ng
            ack.acknowledge();
            
        } catch (Exception e) {
            // Kh√¥ng acknowledge ‚Üí message s·∫Ω ƒë∆∞·ª£c redelivery
            log.error("Failed to process order, will retry", e);
            throw e;
        }
    }
    
    @Transactional
    public void processOrderWithinTransaction(OrderEvent event, long offset) {
        // 1. Check idempotency
        if (offsetAlreadyProcessed(event.getOrderId(), offset)) {
            log.info("Offset {} already processed, skipping", offset);
            return;
        }
        
        // 2. Business logic
        Order order = createOrder(event);
        orderRepository.save(order);
        
        // 3. L∆∞u offset ƒë√£ x·ª≠ l√Ω (trong c√πng transaction)
        ProcessedOffset po = new ProcessedOffset(
            event.getOrderId(), offset, Instant.now()
        );
        processedOffsetRepository.save(po);
        
        // 4. Publish event qua Outbox (transactional)
        outboxService.saveEvent("order.created", order);
    }
}
```

---

## 3. Idempotency: Duplicate Request Handling

### 3.1. Idempotency Key Strategies

| Strategy | Pros | Cons | Use Case |
|----------|------|------|----------|
| **Client-provided UUID** | ƒê∆°n gi·∫£n, client control | Client c√≥ th·ªÉ reuse key | Payment APIs |
| **Hash of request body** | T·ª± ƒë·ªông, kh√¥ng c·∫ßn client | Hash collision, request thay ƒë·ªïi | Batch processing |
| **Composite key** | Ch√≠nh x√°c cho business | Ph·ª©c t·∫°p | Order creation |

### 3.2. Production Idempotency Implementation

```java
@Aspect
@Component
@Order(1) // Ch·∫°y tr∆∞·ªõc @Transactional
public class IdempotencyAspect {
    
    private final IdempotencyKeyStore keyStore;
    private final ObjectMapper objectMapper;
    
    @Around("@annotation(idempotent)")
    public Object handleIdempotency(ProceedingJoinPoint pjp, Idempotent idempotent) 
            throws Throwable {
        
        String idempotencyKey = extractIdempotencyKey(pjp, idempotent);
        String lockKey = "idem:lock:" + idempotencyKey;
        String resultKey = "idem:result:" + idempotencyKey;
        
        // 1. Ki·ªÉm tra k·∫øt qu·∫£ ƒë√£ cache
        Optional<CachedResult> cached = keyStore.get(resultKey);
        if (cached.isPresent()) {
            log.info("Returning cached result for key: {}", idempotencyKey);
            return deserializeResult(cached.get(), pjp.getSignature());
        }
        
        // 2. Acquire distributed lock
        boolean lockAcquired = keyStore.tryLock(lockKey, 
            Duration.ofSeconds(idempotent.lockTimeoutSeconds()));
        
        if (!lockAcquired) {
            // Request ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi instance kh√°c
            throw new IdempotencyConflictException(
                "Request with key " + idempotencyKey + " is being processed"
            );
        }
        
        try {
            // 3. Double-check sau khi c√≥ lock
            cached = keyStore.get(resultKey);
            if (cached.isPresent()) {
                return deserializeResult(cached.get(), pjp.getSignature());
            }
            
            // 4. Execute business logic
            Object result = pjp.proceed();
            
            // 5. Cache result
            CachedResult toCache = CachedResult.builder()
                .result(objectMapper.writeValueAsString(result))
                .resultClass(result.getClass().getName())
                .createdAt(Instant.now())
                .build();
            
            keyStore.set(resultKey, toCache, 
                Duration.ofHours(idempotent.ttlHours()));
            
            return result;
            
        } finally {
            keyStore.releaseLock(lockKey);
        }
    }
    
    private String extractIdempotencyKey(ProceedingJoinPoint pjp, Idempotent idempotent) {
        // ∆Øu ti√™n: Header > Annotation SpEL > Request hash
        HttpServletRequest request = getCurrentRequest();
        
        String headerKey = request.getHeader("Idempotency-Key");
        if (StringUtils.hasText(headerKey)) {
            return headerKey;
        }
        
        if (StringUtils.hasText(idempotent.keyExpression())) {
            return evaluateSpEL(idempotent.keyExpression(), pjp);
        }
        
        // Fallback: hash request
        return hashRequest(pjp.getArgs());
    }
}

// Annotation
@Target(ElementType.METHOD)
@Retention(RetentionPolicy.RUNTIME)
public @interface Idempotent {
    String keyExpression() default "";
    int ttlHours() default 24;
    int lockTimeoutSeconds() default 30;
}

// Usage
@PostMapping("/orders")
@Idempotent(keyExpression = "#request.orderId + ':' + #request.customerId")
public OrderResponse createOrder(@RequestBody CreateOrderRequest request) {
    return orderService.createOrder(request);
}
```

### 3.3. Database-Level Idempotency (Upsert Pattern)

```java
@Repository
public interface OrderRepository extends JpaRepository<Order, Long> {
    
    /**
     * Atomic upsert v·ªõi idempotency key
     * PostgreSQL specific
     */
    @Modifying
    @Query(value = """
        INSERT INTO orders (idempotency_key, customer_id, amount, status, created_at)
        VALUES (:idempotencyKey, :customerId, :amount, 'PENDING', NOW())
        ON CONFLICT (idempotency_key) 
        DO UPDATE SET 
            updated_at = NOW()
        WHERE orders.status = 'PENDING'
        RETURNING *
        """, nativeQuery = true)
    Optional<Order> upsertOrder(
        @Param("idempotencyKey") String idempotencyKey,
        @Param("customerId") Long customerId,
        @Param("amount") BigDecimal amount
    );
}
```

---

## 4. Network Failures & Partial Failures

### 4.1. Timeout Patterns

| Timeout Type | Default | Recommended | Why |
|--------------|---------|-------------|-----|
| **Connection** | 10s | 2-5s | Fast fail n·∫øu kh√¥ng connect ƒë∆∞·ª£c |
| **Read/Socket** | 0 (infinite) | 10-30s | Tr√°nh thread b·ªã block m√£i |
| **Request** | N/A | < Read timeout | T·ªïng th·ªùi gian cho to√†n request |

```java
@Configuration
public class HttpClientConfig {
    
    @Bean
    public RestTemplate restTemplate() {
        RestTemplate template = new RestTemplate();
        
        HttpComponentsClientHttpRequestFactory factory = 
            new HttpComponentsClientHttpRequestFactory();
        
        RequestConfig requestConfig = RequestConfig.custom()
            .setConnectTimeout(Timeout.ofSeconds(3))      // Connect timeout
            .setResponseTimeout(Timeout.ofSeconds(10))    // Read timeout
            .setConnectionRequestTimeout(Timeout.ofSeconds(5)) // Pool wait
            .build();
        
        CloseableHttpClient httpClient = HttpClients.custom()
            .setDefaultRequestConfig(requestConfig)
            .setConnectionManager(createConnectionManager())
            .setRetryStrategy(new DefaultHttpRequestRetryStrategy(3, 
                TimeValue.ofSeconds(1)))
            .build();
        
        factory.setHttpClient(httpClient);
        template.setRequestFactory(factory);
        
        return template;
    }
    
    private PoolingHttpClientConnectionManager createConnectionManager() {
        return PoolingHttpClientConnectionManagerBuilder.create()
            .setMaxConnTotal(200)           // Total connections
            .setMaxConnPerRoute(50)          // Per host
            .setValidateAfterInactivity(TimeValue.ofSeconds(10))
            .build();
    }
}
```

### 4.2. Partial Failure Handling

```java
@Service
@Slf4j
public class OrderAggregatorService {
    
    /**
     * G·ªçi nhi·ªÅu service, x·ª≠ l√Ω partial failure gracefully
     */
    public OrderDetailsResponse getOrderDetails(String orderId) {
        OrderDetailsResponse.OrderDetailsResponseBuilder builder = 
            OrderDetailsResponse.builder();
        
        // Core data - MUST succeed
        Order order = orderService.getOrder(orderId);
        builder.order(order);
        
        // Parallel calls v·ªõi timeout ri√™ng
        CompletableFuture<CustomerInfo> customerFuture = 
            CompletableFuture.supplyAsync(() -> 
                customerService.getCustomer(order.getCustomerId()))
            .orTimeout(2, TimeUnit.SECONDS)
            .exceptionally(ex -> {
                log.warn("Failed to get customer info, using fallback", ex);
                return CustomerInfo.UNKNOWN;
            });
        
        CompletableFuture<List<Review>> reviewsFuture = 
            CompletableFuture.supplyAsync(() ->
                reviewService.getReviews(orderId))
            .orTimeout(3, TimeUnit.SECONDS)
            .exceptionally(ex -> {
                log.warn("Failed to get reviews, returning empty", ex);
                return Collections.emptyList();
            });
        
        CompletableFuture<ShippingStatus> shippingFuture =
            CompletableFuture.supplyAsync(() ->
                shippingService.getStatus(orderId))
            .orTimeout(2, TimeUnit.SECONDS)
            .exceptionally(ex -> {
                log.warn("Failed to get shipping status", ex);
                return ShippingStatus.UNKNOWN;
            });
        
        // Wait for all v·ªõi overall timeout
        try {
            CompletableFuture.allOf(customerFuture, reviewsFuture, shippingFuture)
                .get(5, TimeUnit.SECONDS);
        } catch (TimeoutException e) {
            log.warn("Overall timeout exceeded, partial data available");
        }
        
        return builder
            .customer(customerFuture.getNow(CustomerInfo.UNKNOWN))
            .reviews(reviewsFuture.getNow(Collections.emptyList()))
            .shipping(shippingFuture.getNow(ShippingStatus.UNKNOWN))
            // Indicate partial data
            .dataCompleteness(calculateCompleteness(customerFuture, reviewsFuture, shippingFuture))
            .build();
    }
}
```

---

## 5. Database Edge Cases

### 5.1. Connection Pool Exhaustion

```
Scenario:
1. Service A g·ªçi DB query (10s)
2. Nhi·ªÅu request ƒë·ªìng th·ªùi ‚Üí Pool h·∫øt connection
3. Thread block ch·ªù connection
4. Request timeout ‚Üí Connection kh√¥ng ƒë∆∞·ª£c tr·∫£ l·∫°i ƒë√∫ng c√°ch
5. Pool exhausted ‚Üí Service down!
```

**Gi·∫£i ph√°p: HikariCP Configuration + Monitoring**

```yaml
spring:
  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 3000     # 3s - fail fast
      idle-timeout: 600000         # 10 min
      max-lifetime: 1800000        # 30 min
      leak-detection-threshold: 30000  # 30s - ph√°t hi·ªán connection leak
      
      # Metrics
      register-mbeans: true
```

```java
@Component
@Slf4j
public class HikariMetricsMonitor {
    
    private final HikariDataSource dataSource;
    private final MeterRegistry meterRegistry;
    
    @Scheduled(fixedDelay = 10000)
    public void logPoolMetrics() {
        HikariPoolMXBean pool = dataSource.getHikariPoolMXBean();
        
        int activeConnections = pool.getActiveConnections();
        int idleConnections = pool.getIdleConnections();
        int totalConnections = pool.getTotalConnections();
        int threadsAwaitingConnection = pool.getThreadsAwaitingConnection();
        
        // Alert n·∫øu pool g·∫ßn exhausted
        double utilizationRate = (double) activeConnections / totalConnections;
        
        if (utilizationRate > 0.8) {
            log.warn("Connection pool high utilization: {}%", 
                utilizationRate * 100);
            
            if (threadsAwaitingConnection > 0) {
                log.error("Threads waiting for connection: {}", 
                    threadsAwaitingConnection);
            }
        }
        
        // Record metrics
        meterRegistry.gauge("hikari.connections.active", activeConnections);
        meterRegistry.gauge("hikari.connections.pending", threadsAwaitingConnection);
    }
}
```

### 5.2. Deadlock Detection & Prevention

```java
@Service
@Slf4j
public class DeadlockPreventionService {
    
    /**
     * Lu√¥n lock theo th·ª© t·ª± ƒë·ªÉ tr√°nh deadlock
     */
    @Transactional(timeout = 10)
    public void transferMoney(Long fromAccountId, Long toAccountId, BigDecimal amount) {
        // QUAN TR·ªåNG: Lock theo th·ª© t·ª± ID ƒë·ªÉ tr√°nh deadlock
        Long firstId = Math.min(fromAccountId, toAccountId);
        Long secondId = Math.max(fromAccountId, toAccountId);
        
        Account first = accountRepository.findByIdWithPessimisticLock(firstId);
        Account second = accountRepository.findByIdWithPessimisticLock(secondId);
        
        Account from = fromAccountId.equals(firstId) ? first : second;
        Account to = toAccountId.equals(firstId) ? first : second;
        
        // Business logic
        from.debit(amount);
        to.credit(amount);
        
        accountRepository.saveAll(List.of(from, to));
    }
    
    /**
     * Retry v·ªõi exponential backoff cho deadlock
     */
    @Retryable(
        retryFor = {DeadlockLoserDataAccessException.class},
        maxAttempts = 3,
        backoff = @Backoff(delay = 100, multiplier = 2)
    )
    @Transactional
    public void updateWithRetry(Long id, UpdateRequest request) {
        // Business logic v·ªõi potential deadlock
    }
}
```

### 5.3. N+1 Query Detection

```java
@Configuration
public class QueryCounterConfig {
    
    @Bean
    public HibernatePropertiesCustomizer hibernatePropertiesCustomizer() {
        return properties -> {
            StatisticsImplementor statistics = new StatisticsImpl();
            properties.put("hibernate.stats.factory", 
                () -> statistics);
            properties.put("hibernate.generate_statistics", true);
        };
    }
}

@Aspect
@Component
@Profile("!production")
public class N1QueryDetector {
    
    private static final ThreadLocal<Integer> queryCount = 
        ThreadLocal.withInitial(() -> 0);
    
    @Around("@annotation(org.springframework.web.bind.annotation.GetMapping) || " +
            "@annotation(org.springframework.web.bind.annotation.PostMapping)")
    public Object detectN1(ProceedingJoinPoint pjp) throws Throwable {
        queryCount.set(0);
        
        try {
            Object result = pjp.proceed();
            
            int count = queryCount.get();
            if (count > 10) {
                log.warn("Potential N+1 detected in {}: {} queries",
                    pjp.getSignature().toShortString(), count);
            }
            
            return result;
        } finally {
            queryCount.remove();
        }
    }
}
```

---

## 6. Race Conditions & Concurrency

### 6.1. Double-Submit Prevention

```java
@Service
public class DoubleSubmitGuard {
    
    private final StringRedisTemplate redis;
    
    /**
     * Atomic check-and-lock ƒë·ªÉ prevent double submit
     */
    public <T> T executeOnce(String operationKey, Supplier<T> operation) {
        String lockKey = "submit:lock:" + operationKey;
        
        // Lua script ƒë·ªÉ atomic check v√† set
        String luaScript = """
            if redis.call('exists', KEYS[1]) == 1 then
                return 0
            else
                redis.call('setex', KEYS[1], ARGV[1], '1')
                return 1
            end
            """;
        
        Long acquired = redis.execute(
            new DefaultRedisScript<>(luaScript, Long.class),
            List.of(lockKey),
            "30"  // 30 seconds TTL
        );
        
        if (acquired == 0) {
            throw new DuplicateSubmissionException(
                "Operation already in progress: " + operationKey
            );
        }
        
        try {
            return operation.get();
        } catch (Exception e) {
            // Business logic failed ‚Üí x√≥a lock ƒë·ªÉ c√≥ th·ªÉ retry
            redis.delete(lockKey);
            throw e;
        }
        // Kh√¥ng x√≥a lock sau success ‚Üí prevent re-submit trong TTL
    }
}
```

### 6.2. Optimistic Lock Retry Pattern

```java
@Service
@Slf4j
public class InventoryService {
    
    private static final int MAX_RETRIES = 5;
    private static final long INITIAL_BACKOFF_MS = 50;
    
    /**
     * Retry v·ªõi jitter cho optimistic locking
     */
    public void decrementStock(Long productId, int quantity) {
        int attempt = 0;
        
        while (attempt < MAX_RETRIES) {
            try {
                doDecrementStock(productId, quantity);
                return; // Success
                
            } catch (OptimisticLockingFailureException e) {
                attempt++;
                
                if (attempt >= MAX_RETRIES) {
                    throw new StockUpdateFailedException(
                        "Failed to update stock after " + MAX_RETRIES + " attempts",
                        e
                    );
                }
                
                // Exponential backoff v·ªõi jitter
                long backoff = calculateBackoffWithJitter(attempt);
                log.info("Optimistic lock conflict, retry {} after {}ms", 
                    attempt, backoff);
                
                try {
                    Thread.sleep(backoff);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Interrupted during retry", ie);
                }
            }
        }
    }
    
    private long calculateBackoffWithJitter(int attempt) {
        long exponentialBackoff = INITIAL_BACKOFF_MS * (1L << attempt);
        long jitter = ThreadLocalRandom.current().nextLong(exponentialBackoff / 2);
        return exponentialBackoff + jitter;
    }
    
    @Transactional
    protected void doDecrementStock(Long productId, int quantity) {
        Product product = productRepository.findById(productId)
            .orElseThrow(() -> new ProductNotFoundException(productId));
        
        if (product.getStock() < quantity) {
            throw new InsufficientStockException(productId, product.getStock(), quantity);
        }
        
        product.setStock(product.getStock() - quantity);
        productRepository.save(product); // @Version check t·∫°i ƒë√¢y
    }
}
```

### 6.3. Read-Your-Writes Consistency

```java
@Service
public class ConsistentReadService {
    
    private final JdbcTemplate primaryJdbc;    // Master
    private final JdbcTemplate replicaJdbc;    // Read replica
    private final RedisTemplate<String, String> redis;
    
    /**
     * ƒê·∫£m b·∫£o read-your-writes consistency v·ªõi replication lag
     */
    public Order getOrder(Long orderId, String requesterId) {
        String lastWriteKey = "lastwrite:" + requesterId + ":" + orderId;
        String lastWriteTime = redis.opsForValue().get(lastWriteKey);
        
        if (lastWriteTime != null) {
            Instant writeInstant = Instant.parse(lastWriteTime);
            Instant threshold = Instant.now().minus(5, ChronoUnit.SECONDS);
            
            if (writeInstant.isAfter(threshold)) {
                // Write qu√° g·∫ßn ‚Üí ƒë·ªçc t·ª´ primary
                log.debug("Reading from primary for fresh write");
                return findFromPrimary(orderId);
            }
        }
        
        // Safe ƒë·ªÉ ƒë·ªçc t·ª´ replica
        return findFromReplica(orderId);
    }
    
    @Transactional
    public Order updateOrder(Long orderId, UpdateOrderRequest request, String requesterId) {
        Order order = findFromPrimary(orderId);
        // Update logic...
        Order saved = orderRepository.save(order);
        
        // Track write time
        String lastWriteKey = "lastwrite:" + requesterId + ":" + orderId;
        redis.opsForValue().set(lastWriteKey, Instant.now().toString(), 
            Duration.ofSeconds(30));
        
        return saved;
    }
}
```

---

## 7. API Design: Error Response Patterns

### 7.1. RFC 7807 Problem Details

```java
@RestControllerAdvice
public class GlobalExceptionHandler {
    
    @ExceptionHandler(BusinessException.class)
    public ResponseEntity<ProblemDetail> handleBusinessException(
            BusinessException ex, HttpServletRequest request) {
        
        ProblemDetail problem = ProblemDetail.forStatus(HttpStatus.BAD_REQUEST);
        problem.setType(URI.create("https://api.example.com/errors/" + ex.getErrorCode()));
        problem.setTitle(ex.getTitle());
        problem.setDetail(ex.getMessage());
        problem.setInstance(URI.create(request.getRequestURI()));
        
        // Custom extensions
        problem.setProperty("errorCode", ex.getErrorCode());
        problem.setProperty("timestamp", Instant.now());
        problem.setProperty("traceId", MDC.get("traceId"));
        
        if (ex.getValidationErrors() != null) {
            problem.setProperty("validationErrors", ex.getValidationErrors());
        }
        
        return ResponseEntity
            .status(HttpStatus.BAD_REQUEST)
            .contentType(MediaType.APPLICATION_PROBLEM_JSON)
            .body(problem);
    }
    
    @ExceptionHandler(OptimisticLockingFailureException.class)
    public ResponseEntity<ProblemDetail> handleConcurrency(
            OptimisticLockingFailureException ex, HttpServletRequest request) {
        
        ProblemDetail problem = ProblemDetail.forStatus(HttpStatus.CONFLICT);
        problem.setType(URI.create("https://api.example.com/errors/concurrent-update"));
        problem.setTitle("Concurrent Update Conflict");
        problem.setDetail("The resource was modified by another request. Please retry.");
        problem.setProperty("retryable", true);
        problem.setProperty("suggestedAction", "GET the resource again and retry your update");
        
        return ResponseEntity.status(HttpStatus.CONFLICT).body(problem);
    }
    
    @ExceptionHandler(RateLimitExceededException.class)
    public ResponseEntity<ProblemDetail> handleRateLimit(
            RateLimitExceededException ex) {
        
        ProblemDetail problem = ProblemDetail.forStatus(HttpStatus.TOO_MANY_REQUESTS);
        problem.setTitle("Rate Limit Exceeded");
        problem.setDetail("You have exceeded the request limit. Please wait before retrying.");
        problem.setProperty("retryAfterSeconds", ex.getRetryAfterSeconds());
        problem.setProperty("limit", ex.getLimit());
        problem.setProperty("remaining", 0);
        problem.setProperty("reset", ex.getResetTime());
        
        return ResponseEntity
            .status(HttpStatus.TOO_MANY_REQUESTS)
            .header("Retry-After", String.valueOf(ex.getRetryAfterSeconds()))
            .header("X-RateLimit-Limit", String.valueOf(ex.getLimit()))
            .header("X-RateLimit-Remaining", "0")
            .header("X-RateLimit-Reset", String.valueOf(ex.getResetTime().getEpochSecond()))
            .body(problem);
    }
}
```

### 7.2. Retry-After & Status Code Guide

| Scenario | Status Code | Retry-After | Client Action |
|----------|-------------|-------------|---------------|
| Rate limit | 429 | ‚úì (seconds) | Wait v√† retry |
| Service unavailable | 503 | ‚úì (seconds/date) | Wait v√† retry |
| Conflict/Optimistic lock | 409 | ‚úó | Refresh v√† retry |
| Bad request | 400 | ‚úó | Fix request |
| Server error | 500 | ‚úó | Alert, manual check |
| Gateway timeout | 504 | ‚úó | C√≥ th·ªÉ retry |

---

## 8. Distributed Tracing & Debugging

### 8.1. Context Propagation Pattern

```java
@Configuration
public class TracingConfig {
    
    @Bean
    public Tracer tracer() {
        return Tracer.builder()
            .sampler(Sampler.create(0.1f))  // Sample 10%
            .build();
    }
}

@Component
public class TracingKafkaInterceptor implements ProducerInterceptor<String, Object> {
    
    private final Tracer tracer;
    
    @Override
    public ProducerRecord<String, Object> onSend(ProducerRecord<String, Object> record) {
        Span currentSpan = tracer.currentSpan();
        
        if (currentSpan != null) {
            TraceContext context = currentSpan.context();
            
            record.headers().add("X-Trace-Id", 
                context.traceId().getBytes(StandardCharsets.UTF_8));
            record.headers().add("X-Span-Id", 
                context.spanId().getBytes(StandardCharsets.UTF_8));
            record.headers().add("X-Parent-Span-Id", 
                context.parentId().getBytes(StandardCharsets.UTF_8));
        }
        
        return record;
    }
}

@Service
public class TracingAwareService {
    
    private final Tracer tracer;
    
    public void processWithTracing(String operation, Runnable task) {
        Span span = tracer.newChild(tracer.currentSpan().context())
            .name(operation)
            .start();
        
        try (Tracer.SpanInScope ws = tracer.withSpanInScope(span)) {
            MDC.put("traceId", span.context().traceId());
            MDC.put("spanId", span.context().spanId());
            
            task.run();
            
        } catch (Exception e) {
            span.tag("error", e.getMessage());
            throw e;
        } finally {
            span.finish();
        }
    }
}
```

### 8.2. Structured Logging for Debugging

```java
@Aspect
@Component
@Slf4j
public class DebugLoggingAspect {
    
    private final ObjectMapper objectMapper;
    
    @Around("@annotation(Debuggable)")
    public Object logForDebug(ProceedingJoinPoint pjp) throws Throwable {
        String methodName = pjp.getSignature().toShortString();
        Object[] args = pjp.getArgs();
        
        // Log input (sanitized)
        log.info("‚Üí {} | input: {}", 
            methodName, 
            sanitizeForLogging(args));
        
        long startTime = System.currentTimeMillis();
        
        try {
            Object result = pjp.proceed();
            
            long duration = System.currentTimeMillis() - startTime;
            log.info("‚Üê {} | duration: {}ms | output: {}", 
                methodName, 
                duration,
                sanitizeForLogging(result));
            
            return result;
            
        } catch (Exception e) {
            long duration = System.currentTimeMillis() - startTime;
            log.error("‚úó {} | duration: {}ms | error: {} | input: {}",
                methodName,
                duration,
                e.getMessage(),
                sanitizeForLogging(args));
            throw e;
        }
    }
    
    private String sanitizeForLogging(Object obj) {
        try {
            String json = objectMapper.writeValueAsString(obj);
            // Mask sensitive fields
            return json.replaceAll(
                "\"(password|token|secret|cardNumber)\":\"[^\"]+\"",
                "\"$1\":\"***MASKED***\""
            );
        } catch (Exception e) {
            return obj.toString();
        }
    }
}
```

---

## 9. Chaos Engineering & Testing

### 9.1. Failure Injection Testing

```java
@Component
@ConditionalOnProperty(name = "chaos.enabled", havingValue = "true")
public class ChaosMonkey {
    
    @Value("${chaos.latency.probability:0.1}")
    private double latencyProbability;
    
    @Value("${chaos.latency.max-ms:3000}")
    private long maxLatencyMs;
    
    @Value("${chaos.exception.probability:0.05}")
    private double exceptionProbability;
    
    @Around("@within(org.springframework.stereotype.Service)")
    public Object injectChaos(ProceedingJoinPoint pjp) throws Throwable {
        
        // Random latency injection
        if (ThreadLocalRandom.current().nextDouble() < latencyProbability) {
            long delay = ThreadLocalRandom.current().nextLong(maxLatencyMs);
            log.warn("CHAOS: Injecting {}ms latency into {}", 
                delay, pjp.getSignature().toShortString());
            Thread.sleep(delay);
        }
        
        // Random exception injection
        if (ThreadLocalRandom.current().nextDouble() < exceptionProbability) {
            log.warn("CHAOS: Injecting exception into {}", 
                pjp.getSignature().toShortString());
            throw new RuntimeException("CHAOS: Simulated failure");
        }
        
        return pjp.proceed();
    }
}
```

### 9.2. Integration Testing Patterns

```java
@SpringBootTest
@Testcontainers
@AutoConfigureWireMock(port = 0)
class OrderServiceIntegrationTest {
    
    @Container
    static PostgreSQLContainer<?> postgres = new PostgreSQLContainer<>("postgres:15");
    
    @Container
    static KafkaContainer kafka = new KafkaContainer(
        DockerImageName.parse("confluentinc/cp-kafka:latest"));
    
    @Autowired
    private OrderService orderService;
    
    @Test
    void shouldHandlePaymentServiceTimeout() {
        // Given: Payment service times out
        stubFor(post(urlPathEqualTo("/payments"))
            .willReturn(aResponse()
                .withStatus(200)
                .withFixedDelay(5000))); // 5s delay
        
        // When
        assertThatThrownBy(() -> 
            orderService.createOrder(testOrderRequest()))
            .isInstanceOf(PaymentTimeoutException.class);
        
        // Then: Order should be in PENDING_PAYMENT state
        Order order = orderRepository.findByIdempotencyKey(testIdempotencyKey());
        assertThat(order.getStatus()).isEqualTo(OrderStatus.PENDING_PAYMENT);
        
        // And: Compensation should be triggered
        verify(kafkaTemplate, times(1))
            .send(eq("order-compensation"), any());
    }
    
    @Test
    void shouldRecoverFromDatabaseFailure() {
        // Given: Database temporarily unavailable
        postgres.stop();
        
        // When: Create order fails
        assertThatThrownBy(() -> 
            orderService.createOrder(testOrderRequest()))
            .isInstanceOf(DataAccessException.class);
        
        // Restart database
        postgres.start();
        
        // Then: Retry should succeed
        OrderResponse response = orderService.createOrder(testOrderRequest());
        assertThat(response.getStatus()).isEqualTo("CREATED");
    }
}
```

---

## 10. Production Checklist

### 10.1. Pre-Production Review

| Category | Item | Status |
|----------|------|--------|
| **Idempotency** | T·∫•t c·∫£ write operations c√≥ idempotency key | ‚òê |
| **Timeouts** | Connection, read, request timeout ƒë√£ configure | ‚òê |
| **Retries** | Retry v·ªõi exponential backoff cho transient errors | ‚òê |
| **Circuit Breaker** | External calls c√≥ circuit breaker | ‚òê |
| **DLQ** | Kafka consumers c√≥ DLQ configured | ‚òê |
| **Alerts** | Critical path c√≥ alerting | ‚òê |
| **Metrics** | Business & technical metrics exposed | ‚òê |
| **Tracing** | Distributed tracing enabled | ‚òê |
| **Health Checks** | Liveness & readiness probes | ‚òê |
| **Rate Limiting** | API rate limits configured | ‚òê |

### 10.2. Monitoring Dashboard Essentials

```yaml
# Prometheus alert rules example
groups:
  - name: microservice-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          
      - alert: KafkaConsumerLag
        expr: kafka_consumer_lag_sum > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kafka consumer lag is high"
          
      - alert: DatabaseConnectionPoolExhausted
        expr: hikaricp_connections_pending > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection pool exhausted"
          
      - alert: CircuitBreakerOpen
        expr: resilience4j_circuitbreaker_state{state="open"} == 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Circuit breaker is open"
```

### 10.3. Runbook Template

```markdown
## Incident: [Name]

### Severity Level
- [ ] P1 - Critical: Revenue impact, complete outage
- [ ] P2 - High: Degraded service, partial impact
- [ ] P3 - Medium: Non-critical feature affected
- [ ] P4 - Low: Cosmetic or logging issue

### Detection
- **Alert**: [Alert name]
- **Metric**: [Which metric triggered]
- **Threshold**: [What threshold was crossed]

### Initial Assessment
1. Check Grafana dashboard: [link]
2. Check recent deployments: [link]
3. Check external dependencies: [checklist]

### Mitigation Steps
1. [Step 1]
2. [Step 2]
3. [Rollback procedure if needed]

### Escalation
- On-call: @team-oncall
- Slack channel: #incidents
- PagerDuty: [policy]

### Post-Incident
- [ ] Create incident timeline
- [ ] Write post-mortem
- [ ] Create follow-up tickets
```

---

## üìö Tham kh·∫£o th√™m

| Topic | Resource |
|-------|----------|
| Saga Pattern | [Microservices.io - Saga](https://microservices.io/patterns/data/saga.html) |
| Outbox Pattern | [Debezium Documentation](https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/) |
| Kafka Error Handling | [Confluent - Error Handling](https://www.confluent.io/blog/error-handling-patterns-in-kafka/) |
| Distributed Tracing | [OpenTelemetry Documentation](https://opentelemetry.io/docs/) |
| Chaos Engineering | [Principles of Chaos](https://principlesofchaos.org/) |
| Circuit Breaker | [Resilience4j Documentation](https://resilience4j.readme.io/) |

---

> üí° **Tip Senior/Staff**: Document m·ªçi production incident v√† lessons learned. M·ªói bug l√† m·ªôt c∆° h·ªôi ƒë·ªÉ x√¢y d·ª±ng h·ªá th·ªëng m·∫°nh m·∫Ω h∆°n.

*T√†i li·ªáu n√†y ƒë∆∞·ª£c t·∫°o ƒë·ªÉ h·ªó tr·ª£ qu√° tr√¨nh n√¢ng cao k·ªπ nƒÉng t·ª´ Mid ‚Üí Senior/Staff Engineer.*
